{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:58:54.291427Z",
     "iopub.status.busy": "2022-03-04T17:58:54.290426Z",
     "iopub.status.idle": "2022-03-04T17:58:56.212793Z",
     "shell.execute_reply": "2022-03-04T17:58:56.213793Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:58:56.221799Z",
     "iopub.status.busy": "2022-03-04T17:58:56.219797Z",
     "iopub.status.idle": "2022-03-04T17:58:56.224800Z",
     "shell.execute_reply": "2022-03-04T17:58:56.225801Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:58:56.233807Z",
     "iopub.status.busy": "2022-03-04T17:58:56.232807Z",
     "iopub.status.idle": "2022-03-04T17:58:56.234807Z",
     "shell.execute_reply": "2022-03-04T17:58:56.235808Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:58:56.243814Z",
     "iopub.status.busy": "2022-03-04T17:58:56.241815Z",
     "iopub.status.idle": "2022-03-04T17:58:56.914290Z",
     "shell.execute_reply": "2022-03-04T17:58:56.915292Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import string\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from time import perf_counter\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:58:56.961324Z",
     "iopub.status.busy": "2022-03-04T17:58:56.927300Z",
     "iopub.status.idle": "2022-03-04T17:58:57.372616Z",
     "shell.execute_reply": "2022-03-04T17:58:57.373617Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ayush/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ayush/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ayush/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ayush/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Creating the posting lists from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps involved in Tokenisation and Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loop over all the files and read each the file content.\n",
    "2. Remove the punctuations, remove all non-ascii and all digits.\n",
    "3. Whitespace tokenisation is used.\n",
    "4. Remove all the stop words.\n",
    "5. Append posting for each token in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posting stores the document posting lists, keys: tokens, values: list of document ids containing that token\n",
    "#posting = defaultdict(list)\n",
    "\n",
    "# keys: token, value: number of document containing that element\n",
    "#token_doc_count = defaultdict(int)\n",
    "\n",
    "# keys: doc_id, value: number of words in the document \n",
    "#doc_words_count = defaultdict(int)\n",
    "\n",
    "#path to the corpus\n",
    "#path = 'data\\english-corpora'\n",
    "\n",
    "# to remove all the punctuations\n",
    "#regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# to remove numbers\n",
    "#number_regex = re.compile(r\"\\d\")\n",
    "\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# instance for stemming and lemmatisation\n",
    "#porter = PorterStemmer()\n",
    "#wordnet = WordNetLemmatizer()\n",
    "\n",
    "#i = 0\n",
    "\n",
    "#start = perf_counter()\n",
    "#for filename in os.listdir(path):\n",
    "#    full_path = os.path.join(path, filename)\n",
    "#    doc_id = filename.split('.')[0]\n",
    "    \n",
    "#    with open(full_path, encoding='utf8') as f:\n",
    "        # convert the characters to lower_case\n",
    "#        file_content = f.read().lower()\n",
    "        \n",
    "        # remove the punctuations and non-ascii characters\n",
    "        #file_content = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", file_content)\n",
    "#        file_content = re.sub(r'[^\\w\\s]', '', file_content)\n",
    "#        file_content = re.sub(r'[^\\x00-\\x7f]', \"\", file_content)\n",
    "        \n",
    "        # remove all the numbers\n",
    "        #file_content = re.sub(regex, \"\", file_content)\n",
    "        #file_content = re.sub(r\"\\d\", \"\", file_content)\n",
    "#        file_content = re.sub(\"\\d+\", \"\", file_content)\n",
    "        \n",
    "        # tokeninsing the content of the file\n",
    "#        tokens = word_tokenize(file_content)\n",
    "        \n",
    "#        doc_words_count[doc_id] = len(tokens)\n",
    "        \n",
    "        # removal of stopwords\n",
    "#        tokens = [word for word in tokens if not word in stop_words]\n",
    "        \n",
    "        # get unique tokens in the document\n",
    "#        tokens = list(set(tokens))\n",
    "        \n",
    "        # perform stemming\n",
    "#        tokens = [porter.stem(word) for word in tokens]\n",
    "        \n",
    "        #lemmatisation\n",
    "        #tokens = [wordnet.lemmatize(word) for word in tokens]\n",
    "        \n",
    "#        for token in tokens:\n",
    "#            posting[token].append((doc_id, file_content.count(token)))\n",
    "#            token_doc_count[token] += 1 \n",
    "        \n",
    "        #if i > 10:\n",
    "        #    break\n",
    "        #i += 1\n",
    "        \n",
    "#end = perf_counter()\n",
    "\n",
    "#print(\"Time : {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:58:57.380623Z",
     "iopub.status.busy": "2022-03-04T17:58:57.379622Z",
     "iopub.status.idle": "2022-03-04T17:59:00.619922Z",
     "shell.execute_reply": "2022-03-04T17:59:00.619922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531034\n",
      "<class 'collections.defaultdict'>\n"
     ]
    }
   ],
   "source": [
    "# retieve the posting lists\n",
    "file = \"posting.pkl\"\n",
    "file_obj_new = open(file, 'rb')\n",
    "posting = pickle.load(file_obj_new)\n",
    "print(len(posting))\n",
    "print(type(posting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:00.627929Z",
     "iopub.status.busy": "2022-03-04T17:59:00.625927Z",
     "iopub.status.idle": "2022-03-04T17:59:00.892264Z",
     "shell.execute_reply": "2022-03-04T17:59:00.893283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531034\n",
      "<class 'collections.defaultdict'>\n"
     ]
    }
   ],
   "source": [
    "# retrieve the total_doc_count dictionary\n",
    "file = \"token_doc_count.pkl\"\n",
    "file_obj = open(file, 'rb')\n",
    "token_doc_count = pickle.load(file_obj)\n",
    "print(len(token_doc_count))\n",
    "print(type(token_doc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:00.899269Z",
     "iopub.status.busy": "2022-03-04T17:59:00.897267Z",
     "iopub.status.idle": "2022-03-04T17:59:00.905273Z",
     "shell.execute_reply": "2022-03-04T17:59:00.906274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8635\n",
      "<class 'collections.defaultdict'>\n"
     ]
    }
   ],
   "source": [
    "# retrieve the doc_word_count\n",
    "file = \"doc_word_count.pkl\"\n",
    "file_obj = open(file, 'rb')\n",
    "doc_word_count = pickle.load(file_obj)\n",
    "print(len(doc_word_count))\n",
    "print(type(doc_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the idf array for all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:00.982340Z",
     "iopub.status.busy": "2022-03-04T17:59:00.940329Z",
     "iopub.status.idle": "2022-03-04T17:59:01.290560Z",
     "shell.execute_reply": "2022-03-04T17:59:01.289570Z"
    }
   },
   "outputs": [],
   "source": [
    "# idf dict: key(token) -> value(total docs / number of docs containing that token)\n",
    "idf = defaultdict(int)\n",
    "total_docs = len(posting.keys())\n",
    "\n",
    "for word, doc_cnt in token_doc_count.items():\n",
    "    if doc_cnt != 0:\n",
    "        idf[word] = math.log10(total_docs/doc_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.297564Z",
     "iopub.status.busy": "2022-03-04T17:59:01.296565Z",
     "iopub.status.idle": "2022-03-04T17:59:01.301567Z",
     "shell.execute_reply": "2022-03-04T17:59:01.301567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531034"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the document list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.307571Z",
     "iopub.status.busy": "2022-03-04T17:59:01.305571Z",
     "iopub.status.idle": "2022-03-04T17:59:01.319566Z",
     "shell.execute_reply": "2022-03-04T17:59:01.319566Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_list = list(doc_word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.326572Z",
     "iopub.status.busy": "2022-03-04T17:59:01.325571Z",
     "iopub.status.idle": "2022-03-04T17:59:01.329573Z",
     "shell.execute_reply": "2022-03-04T17:59:01.329573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8635"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.337595Z",
     "iopub.status.busy": "2022-03-04T17:59:01.336578Z",
     "iopub.status.idle": "2022-03-04T17:59:01.339831Z",
     "shell.execute_reply": "2022-03-04T17:59:01.340834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2875.190503763752"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_doc_size = sum(list(doc_word_count.values())) / len(doc_word_count)\n",
    "avg_doc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.347842Z",
     "iopub.status.busy": "2022-03-04T17:59:01.346841Z",
     "iopub.status.idle": "2022-03-04T17:59:01.351841Z",
     "shell.execute_reply": "2022-03-04T17:59:01.351841Z"
    }
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Creating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.368872Z",
     "iopub.status.busy": "2022-03-04T17:59:01.355845Z",
     "iopub.status.idle": "2022-03-04T17:59:01.371875Z",
     "shell.execute_reply": "2022-03-04T17:59:01.372876Z"
    }
   },
   "outputs": [],
   "source": [
    "def BooleanRetrieval(query, n=None):\n",
    "    # preprocess the query\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "    query = re.sub(r'[^\\x00-\\x7f]', \"\", query)\n",
    "    query_tokens = word_tokenize(query)\n",
    "\n",
    "    query_tokens = [porter.stem(word) for word in query_tokens if word != 'and']\n",
    "\n",
    "    # get unique elements in the list\n",
    "    query_tokens = list(set(query_tokens))\n",
    "    #print(query_tokens)\n",
    "\n",
    "    # retrieve the model\n",
    "    rel_docs = set()\n",
    "    for word in query_tokens:\n",
    "        if len(posting[word]) == 0:\n",
    "            return list()\n",
    "        p = set([w[0] for w in posting[word]])\n",
    "        if len(rel_docs) == 0:\n",
    "            rel_docs = p\n",
    "        else:\n",
    "            rel_docs = rel_docs & p\n",
    "    \n",
    "    if n == None or n >= len(posting):     \n",
    "        return list(rel_docs)\n",
    "    return list(rel_docs)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.382883Z",
     "iopub.status.busy": "2022-03-04T17:59:01.381866Z",
     "iopub.status.idle": "2022-03-04T17:59:01.386886Z",
     "shell.execute_reply": "2022-03-04T17:59:01.386886Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_tf_idf(query, n = None):\n",
    "    # preprocess the query\n",
    "    query = query.lower()\n",
    "    \n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "    query = re.sub(r'[^\\x00-\\x7f]', \"\", query)\n",
    "    query_tokens = word_tokenize(query)\n",
    "    \n",
    "    query_tokens = [word for word in query_tokens if not word in stop_words]\n",
    "    \n",
    "    query_tokens = [porter.stem(word) for word in query_tokens if word != 'and']\n",
    "    \n",
    "\n",
    "    # get unique elements in the list\n",
    "    query_tokens = list(set(query_tokens))\n",
    "    #print(query_tokens)\n",
    "    \n",
    "    # compute tf-idf score and retreive the document using cosine similarity\n",
    "    cosine_sim = defaultdict(float)\n",
    "    for q_token in query_tokens:\n",
    "        for doc, count in posting[q_token]:\n",
    "            cosine_sim[doc] += count * idf[q_token]\n",
    "    \n",
    "    tf_idf_list = list(cosine_sim.items())\n",
    "    tf_idf_list.sort(key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    rel_docs = [word[0] for word in tf_idf_list]\n",
    "    \n",
    "    if n is None or n >= len(tf_idf_list):\n",
    "        return rel_docs\n",
    "    return rel_docs[:n]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.412885Z",
     "iopub.status.busy": "2022-03-04T17:59:01.410884Z",
     "iopub.status.idle": "2022-03-04T17:59:01.413886Z",
     "shell.execute_reply": "2022-03-04T17:59:01.414886Z"
    }
   },
   "outputs": [],
   "source": [
    "def bm25_scores(query, n=None):\n",
    "\n",
    "    k = 2\n",
    "    b = 0.75\n",
    "    \n",
    "    # preprocess the query\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "    query = re.sub(r'[^\\x00-\\x7f]', \"\", query)\n",
    "    query_tokens = word_tokenize(query)\n",
    "\n",
    "    query_tokens = [porter.stem(word) for word in query_tokens if word != 'and']\n",
    "\n",
    "    # get unique elements in the list\n",
    "    query_tokens = list(set(query_tokens))\n",
    "    \n",
    "    # calculate the bm25 scores for each of the document\n",
    "    bm25 = defaultdict(float)\n",
    "    for q in query_tokens:\n",
    "        for doc in doc_list:\n",
    "            tf = [element[1] for element in posting[q] if element[0] == doc]\n",
    "            \n",
    "            # if the current document doesn't contain the query term, don't need to rank it\n",
    "            if len(tf) == 0:\n",
    "                continue\n",
    "            if bm25[doc] == float(0):\n",
    "                bm25[doc] = idf[q] * ((tf[0] * (k+1))/(tf[0] + k*(1 - b + b*avg_doc_size)))\n",
    "            else:\n",
    "                bm25[doc] += idf[q] * ((tf[0] * (k+1))/(tf[0] + k*(1 - b + b*avg_doc_size)))\n",
    "                \n",
    "    bm25_list = list(bm25.items())\n",
    "    bm25_list.sort(key = lambda x : x[1], reverse=True)\n",
    "    \n",
    "    rel_docs = [word[0] for word in bm25_list]\n",
    "    \n",
    "    if n is None or n >= len(bm25_list):\n",
    "        return rel_docs\n",
    "    \n",
    "    return rel_docs[:n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.420910Z",
     "iopub.status.busy": "2022-03-04T17:59:01.419890Z",
     "iopub.status.idle": "2022-03-04T17:59:01.423893Z",
     "shell.execute_reply": "2022-03-04T17:59:01.423893Z"
    }
   },
   "outputs": [],
   "source": [
    "#query_set_df = pd.read_csv('query_set.tsv', sep='\\t')\n",
    "#query_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.430898Z",
     "iopub.status.busy": "2022-03-04T17:59:01.429897Z",
     "iopub.status.idle": "2022-03-04T17:59:01.432899Z",
     "shell.execute_reply": "2022-03-04T17:59:01.432899Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = sys.argv[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.438903Z",
     "iopub.status.busy": "2022-03-04T17:59:01.437903Z",
     "iopub.status.idle": "2022-03-04T17:59:01.442915Z",
     "shell.execute_reply": "2022-03-04T17:59:01.441913Z"
    }
   },
   "outputs": [],
   "source": [
    "queries = []\n",
    "with open(\"query_set.tsv\") as file:\n",
    "    query_set_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    for row in query_set_file:\n",
    "        queries.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T17:59:01.457924Z",
     "iopub.status.busy": "2022-03-04T17:59:01.455929Z",
     "iopub.status.idle": "2022-03-04T18:00:00.978274Z",
     "shell.execute_reply": "2022-03-04T18:00:00.979275Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. loop over all the queries\n",
    "# 2. run query in all 3 models, each returning top 5 document\n",
    "# 3. Write the output document list in Qrels format in seperate 3 files\n",
    "\n",
    "qrels_bool, qrels_tfidf, qrels_bm25 = [], [], []\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for query in queries[1:]:\n",
    "    qid = query[0]\n",
    "    text = query[1]\n",
    "    \n",
    "    rel_docs_bool = BooleanRetrieval(text, n=5)\n",
    "    for doc in rel_docs_bool:\n",
    "        qrels_bool.append([qid, '1', doc, '1'])\n",
    "        \n",
    "    rel_docs_tfidf = compute_tf_idf(text, n=5)\n",
    "    for doc in rel_docs_tfidf:\n",
    "        qrels_tfidf.append([qid, '1', doc, '1'])\n",
    "        \n",
    "    rel_docs_bm25 = bm25_scores(text, n=5)\n",
    "    for doc in rel_docs_bm25:\n",
    "        qrels_bm25.append([qid, '1', doc, '1'])\n",
    "        \n",
    "qrels_df_bool = pd.DataFrame(qrels_bool)\n",
    "qrels_df_bool.to_csv('QRels_boolean.csv', index=False, header=False)\n",
    "\n",
    "qrels_df_tfidf = pd.DataFrame(qrels_tfidf)\n",
    "qrels_df_tfidf.to_csv('QRels_tfidf.csv', index=False, header=False)\n",
    "\n",
    "qrels_df_bm25 = pd.DataFrame(qrels_bm25)\n",
    "qrels_df_bm25.to_csv('QRels_bm25.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
